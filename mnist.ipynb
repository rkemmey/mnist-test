{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7a1ff99ff0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchmetrics import Accuracy\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "torch.manual_seed(42) # set manual seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset info\n",
    "\n",
    "- https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html\n",
    "- Can also be found on kaggle: https://www.kaggle.com/datasets/hojjatk/mnist-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data and view an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset contains the pictures as <class 'PIL.Image.Image'>\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28>, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contains the image and a number as a tuple\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split up tuple to see image\n",
    "train_image_zero, train_target_zero = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCABkAGQBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+iiiiiiiiiiiiiiiiiiiiiiiiiiiinxxSSnCIzH2FaWn+HtQ1F9sMLZ9xXTWPwp8QX06xJGAW9q2z8BPE+RjZXYad8DtlpELuJTKPvGszxP8ACOJNSso9OVFQkCQZr0HTfhB4UtLBDf20by4yTkCvH/ihovhzSJpIdJiVWHTFeV0UUUV7L8ELbRr/AFCaLUYI3IHG+vfoPD/h+3YGG0t0J6YrUisraBg0cSqR3ArOv/Ethp8xilnjDDsWqw2q2s2my3ENxG2Iy3DdOK+XdZ+JV8ut3QEr4SQhefQ1mXfxO1q5GBdyAfWuT1HVLrUp2luJWcn1qlRRRRWpomt3GiXXnW7MD3wa7uz+Kmo3OsWI+farAEZ619GLrs1x4UfUooiZFTIUfSvlDxtr+pX3iW4lklliOfuBiMUaD491PRY5YzLJMrrtwzdK5i6nNzdSzkYMjFj+NRUUUUUUUVLbztbXCTJ95DkV9JfCPx5Dq9guk37qJW+UL61xnxX+HuoDxFPe2NsWibngV5Pe6XeaeR9phKZ9ap0UUUUUUUUVp6Dq8+iarDewMVdDmvpvwb8RtH8UafHZ37K10Rg5qz43+G2n6/pe6yiXzBzkV80eJvC954fvGjmjIXOBxXP0UUUUUUUUVb0++vLCcS2bukg7rX0J8E/F+qa3fTWV8ZSI0zluld7458Jadq2iXUrwoJVQkMRXx9qVmbK9kh3BsMRxVSiiiiiilALEADJNdDoPhDUtbuVRLeQRk/exXtnhT4JQ26JPdSZPcMK9S0PwxYeHt0kEaIcYLAYrF8feI9Og8P3UK30YlZCNoavj64kaS4kZmLEseT9aiooooorodN8Fa5qaJJb2TtG3RsV6r4F+DDXDedrEbRMvIBFe36N4b0/Qrby4I1IA6kVW1nxpouixP592iyD+GvFvF3xvvC722nbGjPG4V49qut3mr3LTXEr5JzjdxWbRRRRVzS7JtQ1CO2Xq5r6F8I/BjSrnTobq/jUseeK9XsdI03QbJY40RI4xxmqdz420a1jkeScBY+vNeU+NvjXbPBJDok53YxXh2r+INQ1qUyXkxYnrzWVRRRRRRUtvcS2syywuVdehFdDD8QfE9vEIotVmVB0ANEvxA8Tzrtk1WZh6ZrLl1/U50ZZLp2DdRnrWbRRRRRRRRRRRRRRRRRRRRX//2Q==",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=100x100>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize image because original is small\n",
    "resized_image = train_image_zero.resize((100, 100))\n",
    "\n",
    "#train_image_zero\n",
    "resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see what's in test dataset\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform images to tensors\n",
    "- use torchvision.transforms to convert images to tensors & transforms.Compose allows you to stack multiple transformations at once\n",
    "    - link: https://pytorch.org/vision/0.9/transforms.html \n",
    "- if you try to apply ToTensor() on the train_dataset you will run into:\n",
    "    - you get an <AttributeError: 'MNIST' object has no attribute 'torchvision'>\n",
    "    - <TypeError: ToTensor.__init__() takes 1 positional argument but 2 were given> because tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformations\n",
    "transformations = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,), (0.5,))])  # using 0.5, 0.5, on many datasets will rearrange them to [-1, +1]\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transformations)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f793686c7a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(train_loader)  # Create an iterator\n",
    "images, labels = next(data_iter)  # Get the next batch\n",
    "images.shape\n",
    "#labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helpful link: \n",
    "- https://www.cs.toronto.edu/~lczhang/321/tut/tut04.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassificationModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MultiClassificationModel, self).__init__()\n",
    "    self.lin1 = nn.Linear(784, 128)  #784 input features due to 28x28 pixel image\n",
    "    self.dp1 = nn.Dropout(p=0.5)\n",
    "    self.lin2 = nn.Linear(128, 10)\n",
    "    self.act = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(x.size(0), -1)  # Flatten from [batch_size, 1, 28, 28] to [batch_size, 784]\n",
    "    x = self.dp1(F.relu(self.lin1(x)))\n",
    "    x = self.act(self.lin2(x))\n",
    "    return x\n",
    "  \n",
    "model = MultiClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss 1.5724898040421735, Epoch: 1\n",
      "Average Loss 1.565006990422572, Epoch: 2\n"
     ]
    }
   ],
   "source": [
    "# loss function and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for images, labels in iter(train_loader):\n",
    "        output = model(images) # model.forward()\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.item() # keep track of overall loss rate\n",
    "    avg_loss = running_loss/len(train_loader)\n",
    "    #if not(epoch+1) % 10:\n",
    "    print(f\"Average Loss {avg_loss}, Epoch: {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9305999875068665\n"
     ]
    }
   ],
   "source": [
    "acc = Accuracy(task='multiclass', num_classes=10)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output.data, 1) \n",
    "        acc.update(predicted, labels)\n",
    "\n",
    "print(f\"Accuracy: {acc.compute().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
